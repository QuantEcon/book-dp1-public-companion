{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e8e5d3e",
   "metadata": {},
   "source": [
    "(Chapter 5: Markov Decision Processes)=\n",
    "```{raw} jupyter\n",
    "<div id=\"qe-notebook-header\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>\n",
    "```\n",
    "# Chapter 5: Markov Decision Processes\n",
    "\n",
    "\n",
    "```{contents} Contents\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### inventory_dp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106790a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from quantecon import compute_fixed_point\n",
    "\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from numba import njit\n",
    "\n",
    "\n",
    "# NamedTuple Model\n",
    "Model = namedtuple(\"Model\", (\"β\", \"K\", \"c\", \"κ\", \"p\"))\n",
    "\n",
    "\n",
    "def create_inventory_model(β=0.98,      # discount factor\n",
    "                           K=40,        # maximum inventory\n",
    "                           c=0.2, κ=2,  # cost parameters\n",
    "                           p=0.6):      # demand parameter\n",
    "    return Model(β=β, K=K, c=c, κ=κ, p=p)\n",
    "\n",
    "\n",
    "@njit\n",
    "def demand_pdf(d, p):\n",
    "    return (1 - p)**d * p\n",
    "\n",
    "\n",
    "@njit\n",
    "def B(x, a, v, model, d_max=101):\n",
    "    \"\"\"\n",
    "    The function B(x, a, v) = r(x, a) + β Σ_x′ v(x′) P(x, a, x′).\n",
    "    \"\"\"\n",
    "    β, K, c, κ, p = model\n",
    "    x1 = np.array([np.minimum(x, d)*demand_pdf(d, p) for d in np.arange(d_max)])\n",
    "    reward = np.sum(x1) - c * a - κ * (a > 0)\n",
    "    x2 = np.array([v[np.maximum(0, x - d) + a] * demand_pdf(d, p)\n",
    "                                 for d in np.arange(d_max)])\n",
    "    continuation_value = β * np.sum(x2)\n",
    "    return reward + continuation_value\n",
    "\n",
    "\n",
    "@njit\n",
    "def T(v, model):\n",
    "    \"\"\"The Bellman operator.\"\"\"\n",
    "    β, K, c, κ, p = model\n",
    "    new_v = np.empty_like(v)\n",
    "    for x in range(0, K+1):\n",
    "        x1 = np.array([B(x, a, v, model) for a in np.arange(K-x+1)])\n",
    "        new_v[x] = np.max(x1)\n",
    "    return new_v\n",
    "\n",
    "\n",
    "@njit\n",
    "def get_greedy(v, model):\n",
    "    \"\"\"\n",
    "    Get a v-greedy policy.  Returns a zero-based array.\n",
    "    \"\"\"\n",
    "    β, K, c, κ, p = model\n",
    "    σ_star = np.zeros(K+1, dtype=np.int32)\n",
    "    for x in range(0, K+1):\n",
    "        x1 = np.array([B(x, a, v, model) for a in np.arange(K-x+1)])\n",
    "        σ_star[x] = np.argmax(x1)\n",
    "    return σ_star\n",
    "\n",
    "\n",
    "def solve_inventory_model(v_init, model):\n",
    "    \"\"\"Use successive_approx to get v_star and then compute greedy.\"\"\"\n",
    "    β, K, c, κ, p = model\n",
    "    v_star = compute_fixed_point(lambda v: T(v, model), v_init,\n",
    "                                 error_tol=1e-5, max_iter=1000, print_skip=25)\n",
    "    σ_star = get_greedy(v_star, model)\n",
    "    return v_star, σ_star\n",
    "\n",
    "\n",
    "# == Plots == #\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"text.usetex\": True, \"font.size\": 14})\n",
    "\n",
    "\n",
    "# Create an instance of the model and solve it\n",
    "model = create_inventory_model()\n",
    "β, K, c, κ, p = model\n",
    "v_init = np.zeros(K+1)\n",
    "v_star, σ_star = solve_inventory_model(v_init, model)\n",
    "\n",
    "\n",
    "def sim_inventories(ts_length=400, X_init=0):\n",
    "    \"\"\"Simulate given the optimal policy.\"\"\"\n",
    "    global p, σ_star\n",
    "    X = np.zeros(ts_length, dtype=np.int32)\n",
    "    X[0] = X_init\n",
    "    # Subtracts 1 because numpy generates only positive integers\n",
    "    rand = np.random.default_rng().geometric(p=p, size=ts_length-1) - 1\n",
    "    for t in range(0, ts_length-1):\n",
    "        X[t+1] = np.maximum(X[t] - rand[t], 0) + σ_star[X[t] + 1]\n",
    "    return X\n",
    "\n",
    "\n",
    "def plot_vstar_and_opt_policy(fontsize=10,\n",
    "                   figname=\"../figures/inventory_dp_vs.pdf\",\n",
    "                   savefig=False):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(8, 6.5))\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.plot(np.arange(K+1), v_star, label=r\"$v^*$\")\n",
    "    ax.set_ylabel(\"value\", fontsize=fontsize)\n",
    "    ax.legend(fontsize=fontsize, frameon=False)\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.plot(np.arange(K+1), σ_star, label=r\"$\\sigma^*$\")\n",
    "    ax.set_xlabel(\"inventory\", fontsize=fontsize)\n",
    "    ax.set_ylabel(\"optimal choice\", fontsize=fontsize)\n",
    "    ax.legend(fontsize=fontsize, frameon=False)\n",
    "    if savefig:\n",
    "        fig.savefig(figname)\n",
    "\n",
    "\n",
    "def plot_ts(fontsize=10,\n",
    "            figname=\"../figures/inventory_dp_ts.pdf\",\n",
    "            savefig=False):\n",
    "    X = sim_inventories()\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.5))\n",
    "    ax.plot(X, label=\"$X_t$\", alpha=0.7)\n",
    "    ax.set_xlabel(\"$t$\", fontsize=fontsize)\n",
    "    ax.set_ylabel(\"inventory\", fontsize=fontsize)\n",
    "    ax.legend(fontsize=fontsize, frameon=False)\n",
    "    ax.set_ylim(0, np.max(X)+4)\n",
    "    if savefig:\n",
    "        fig.savefig(figname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc15ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vstar_and_opt_policy(savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts(savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370e56e",
   "metadata": {},
   "source": [
    "#### finite_opt_saving_0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b2bb0",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from quantecon.markov import tauchen\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from numba import njit, prange\n",
    "\n",
    "\n",
    "# NamedTuple Model\n",
    "Model = namedtuple(\"Model\", (\"β\", \"R\", \"γ\", \"w_grid\", \"y_grid\", \"Q\"))\n",
    "\n",
    "\n",
    "def create_savings_model(R=1.01, β=0.98, γ=2.5,\n",
    "                         w_min=0.01, w_max=20.0, w_size=200,\n",
    "                         ρ=0.9, ν=0.1, y_size=5):\n",
    "    w_grid = np.linspace(w_min, w_max, w_size)\n",
    "    mc = tauchen(y_size, ρ, ν)\n",
    "    y_grid, Q = np.exp(mc.state_values), mc.P\n",
    "    return Model(β=β, R=R, γ=γ, w_grid=w_grid, y_grid=y_grid, Q=Q)\n",
    "\n",
    "\n",
    "@njit\n",
    "def U(c, γ):\n",
    "    return c**(1-γ)/(1-γ)\n",
    "\n",
    "\n",
    "@njit\n",
    "def B(i, j, k, v, model):\n",
    "    \"\"\"\n",
    "    B(w, y, w′, v) = u(R*w + y - w′) + β Σ_y′ v(w′, y′) Q(y, y′).\n",
    "    \"\"\"\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    w, y, w_1 = w_grid[i], y_grid[j], w_grid[k]\n",
    "    c = w + y - (w_1 / R)\n",
    "    value = -np.inf\n",
    "    if c > 0:\n",
    "        value = U(c, γ) + β * np.dot(v[k, :], Q[j, :])\n",
    "    return value\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def T(v, model):\n",
    "    \"\"\"The Bellman operator.\"\"\"\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    v_new = np.empty_like(v)\n",
    "    for i in prange(w_grid.shape[0]):\n",
    "        for j in prange(y_grid.shape[0]):\n",
    "            x_tmp = np.array([B(i, j, k, v, model) for k\n",
    "                              in np.arange(w_grid.shape[0])])\n",
    "            v_new[i, j] = np.max(x_tmp)\n",
    "    return v_new\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def T_σ(v, σ, model):\n",
    "    \"\"\"The policy operator.\"\"\"\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    v_new = np.empty_like(v)\n",
    "    for i in prange(w_grid.shape[0]):\n",
    "        for j in prange(y_grid.shape[0]):\n",
    "            v_new[i, j] = B(i, j, σ[i, j], v, model)\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e720e134",
   "metadata": {},
   "source": [
    "#### finite_opt_saving_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da98483",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from finite_opt_saving_0 import U, B\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_greedy(v, model):\n",
    "    \"\"\"Compute a v-greedy policy.\"\"\"\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    σ = np.empty((w_grid.shape[0], y_grid.shape[0]), dtype=np.int32)\n",
    "    for i in prange(w_grid.shape[0]):\n",
    "        for j in range(y_grid.shape[0]):\n",
    "            x_tmp = np.array([B(i, j, k, v, model) for k in\n",
    "                             np.arange(w_grid.shape[0])])\n",
    "            σ[i, j] = np.argmax(x_tmp)\n",
    "    return σ\n",
    "\n",
    "\n",
    "@njit\n",
    "def single_to_multi(m, yn):\n",
    "    # Function to extract (i, j) from m = i + (j-1)*yn\n",
    "    return (m//yn, m%yn)\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_value(σ, model):\n",
    "    \"\"\"Get the value v_σ of policy σ.\"\"\"\n",
    "    # Unpack and set up\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    wn, yn = len(w_grid), len(y_grid)\n",
    "    n = wn * yn\n",
    "    # Build P_σ and r_σ as multi-index arrays\n",
    "    P_σ = np.zeros((wn, yn, wn, yn))\n",
    "    r_σ = np.zeros((wn, yn))\n",
    "    for i in range(wn):\n",
    "        for j in range(yn):\n",
    "            w, y, w_1 = w_grid[i], y_grid[j], w_grid[σ[i, j]]\n",
    "            r_σ[i, j] = U(w + y - w_1/R, γ)\n",
    "            for i_1 in range(wn):\n",
    "                for j_1 in range(yn):\n",
    "                    if i_1 == σ[i, j]:\n",
    "                        P_σ[i, j, i_1, j_1] = Q[j, j_1]\n",
    "\n",
    "    # Solve for the value of σ\n",
    "    P_σ = P_σ.reshape(n, n)\n",
    "    r_σ = r_σ.reshape(n)\n",
    "\n",
    "    I = np.identity(n)\n",
    "    v_σ = np.linalg.solve((I - β * P_σ), r_σ)\n",
    "    # Return as multi-index array\n",
    "    v_σ = v_σ.reshape(wn, yn)\n",
    "    return v_σ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0426e391",
   "metadata": {},
   "source": [
    "#### finite_opt_saving_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b94b0d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from quantecon import compute_fixed_point\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "import time\n",
    "from finite_opt_saving_1 import get_greedy, get_value\n",
    "from finite_opt_saving_0 import create_savings_model, T, T_σ\n",
    "from quantecon import MarkovChain\n",
    "\n",
    "\n",
    "def value_iteration(model, tol=1e-5):\n",
    "    \"\"\"Value function iteration routine.\"\"\"\n",
    "    vz = np.zeros((len(model.w_grid), len(model.y_grid)))\n",
    "    v_star = compute_fixed_point(lambda v: T(v, model), vz,\n",
    "                                 error_tol=tol, max_iter=1000, print_skip=25)\n",
    "    return get_greedy(v_star, model)\n",
    "\n",
    "\n",
    "@njit(cache=True, fastmath=True)\n",
    "def policy_iteration(model):\n",
    "    \"\"\"Howard policy iteration routine.\"\"\"\n",
    "    wn, yn = len(model.w_grid), len(model.y_grid)\n",
    "    σ = np.ones((wn, yn), dtype=np.int32)\n",
    "    i, error = 0, 1.0\n",
    "    while error > 0:\n",
    "        v_σ = get_value(σ, model)\n",
    "        σ_new = get_greedy(v_σ, model)\n",
    "        error = np.max(np.abs(σ_new - σ))\n",
    "        σ = σ_new\n",
    "        i = i + 1\n",
    "        print(f\"Concluded loop {i} with error: {error}.\")\n",
    "    return σ\n",
    "\n",
    "\n",
    "@njit\n",
    "def optimistic_policy_iteration(model, tolerance=1e-5, m=100):\n",
    "    \"\"\"Optimistic policy iteration routine.\"\"\"\n",
    "    v = np.zeros((len(model.w_grid), len(model.y_grid)))\n",
    "    error = tolerance + 1\n",
    "    while error > tolerance:\n",
    "        last_v = v\n",
    "        σ = get_greedy(v, model)\n",
    "        for i in range(0, m):\n",
    "            v = T_σ(v, σ, model)\n",
    "        error = np.max(np.abs(v - last_v))\n",
    "    return get_greedy(v, model)\n",
    "\n",
    "# Simulations and inequality measures\n",
    "\n",
    "def simulate_wealth(m):\n",
    "\n",
    "    model = create_savings_model()\n",
    "    σ_star = optimistic_policy_iteration(model)\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "\n",
    "    # Simulate labor income (indices rather than grid values)\n",
    "    mc = MarkovChain(Q)\n",
    "    y_idx_series = mc.simulate(ts_length=m)\n",
    "\n",
    "    # Compute corresponding wealth time series\n",
    "    w_idx_series = np.empty_like(y_idx_series)\n",
    "    w_idx_series[0] = 1  # initial condition\n",
    "    for t in range(m-1):\n",
    "        i, j = w_idx_series[t], y_idx_series[t]\n",
    "        w_idx_series[t+1] = σ_star[i, j]\n",
    "    w_series = w_grid[w_idx_series]\n",
    "\n",
    "    return w_series\n",
    "\n",
    "def lorenz(v):  # assumed sorted vector\n",
    "    S = np.cumsum(v)  # cumulative sums: [v[1], v[1] + v[2], ... ]\n",
    "    F = np.arange(1, len(v) + 1) / len(v)\n",
    "    L = S / S[-1]\n",
    "    return (F, L) # returns named tuple\n",
    "\n",
    "gini = lambda v: (2 * sum(i * y for (i, y) in enumerate(v))/sum(v) - (len(v) + 1))/len(v)\n",
    "\n",
    "# Plots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"text.usetex\": True, \"font.size\": 14})\n",
    "\n",
    "\n",
    "def plot_timing(m_vals=np.arange(1, 601, 10),\n",
    "                savefig=False):\n",
    "    model = create_savings_model(y_size=5)\n",
    "    print(\"Running Howard policy iteration.\")\n",
    "    t1 = time.time()\n",
    "    σ_pi = policy_iteration(model)\n",
    "    pi_time = time.time() - t1\n",
    "    print(f\"PI completed in {pi_time} seconds.\")\n",
    "    print(\"Running value function iteration.\")\n",
    "    t1 = time.time()\n",
    "    σ_vfi = value_iteration(model)\n",
    "    vfi_time = time.time() - t1\n",
    "    print(f\"VFI completed in {vfi_time} seconds.\")\n",
    "\n",
    "    assert np.allclose(σ_vfi, σ_pi), \"Warning: policies deviated.\"\n",
    "\n",
    "    opi_times = []\n",
    "    for m in m_vals:\n",
    "        print(f\"Running optimistic policy iteration with m={m}.\")\n",
    "        t1 = time.time()\n",
    "        σ_opi = optimistic_policy_iteration(model, m=m)\n",
    "        t2 = time.time()\n",
    "        assert np.allclose(σ_opi, σ_pi), \"Warning: policies deviated.\"\n",
    "        print(f\"OPI with m={m} completed in {t2-t1} seconds.\")\n",
    "        opi_times.append(t2-t1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "    ax.plot(m_vals, [vfi_time]*len(m_vals),\n",
    "            linewidth=2, label=\"value function iteration\")\n",
    "    ax.plot(m_vals, [pi_time]*len(m_vals),\n",
    "            linewidth=2, label=\"Howard policy iteration\")\n",
    "    ax.plot(m_vals, opi_times, linewidth=2,\n",
    "            label=\"optimistic policy iteration\")\n",
    "    ax.legend(frameon=False)\n",
    "    ax.set_xlabel(r\"$m$\")\n",
    "    ax.set_ylabel(\"time\")\n",
    "    if savefig:\n",
    "        fig.savefig(\"../figures/finite_opt_saving_2_1.png\")\n",
    "    return (pi_time, vfi_time, opi_times)\n",
    "\n",
    "\n",
    "def plot_policy(method=\"pi\", savefig=False):\n",
    "    model = create_savings_model()\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    if method == \"vfi\":\n",
    "        σ_star =  value_iteration(model)\n",
    "    elif method == \"pi\":\n",
    "        σ_star = policy_iteration(model)\n",
    "    else:\n",
    "        method = \"OPT\"\n",
    "        σ_star = optimistic_policy_iteration(model)\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "    ax.plot(w_grid, w_grid, \"k--\", label=r\"$45$\")\n",
    "    ax.plot(w_grid, w_grid[σ_star[:, 0]], label=r\"$\\sigma^*(\\cdot, y_1)$\")\n",
    "    ax.plot(w_grid, w_grid[σ_star[:, -1]], label=r\"$\\sigma^*(\\cdot, y_N)$\")\n",
    "    ax.legend()\n",
    "    plt.title(f\"Method: {method}\")\n",
    "    if savefig:\n",
    "        fig.savefig(f\"../figures/finite_opt_saving_2_2_{method}.png\")\n",
    "\n",
    "def plot_time_series(m=2_000, savefig=False):\n",
    "\n",
    "    w_series = simulate_wealth(m)\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "    ax.plot(w_series, label=\"w_t\")\n",
    "    ax.set_xlabel(\"time\")\n",
    "    ax.legend()\n",
    "    if savefig:\n",
    "        fig.savefig(\"../figures/finite_opt_saving_ts.pdf\")\n",
    "\n",
    "def plot_histogram(m=1_000_000, savefig=False):\n",
    "\n",
    "    w_series = simulate_wealth(m)\n",
    "    w_series.sort()\n",
    "    g = round(gini(w_series), ndigits=2)\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "    ax.hist(w_series, bins=40, density=True)\n",
    "    ax.set_xlabel(\"wealth\")\n",
    "    ax.text(15, 0.4, f\"Gini = {g}\")\n",
    "\n",
    "    if savefig:\n",
    "        fig.savefig(\"../figures/finite_opt_saving_hist.pdf\")\n",
    "\n",
    "def plot_lorenz(m=1_000_000, savefig=False):\n",
    "\n",
    "    w_series = simulate_wealth(m)\n",
    "    w_series.sort()\n",
    "    (F, L) = lorenz(w_series)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "    ax.plot(F, F, label=\"Lorenz curve, equality\")\n",
    "    ax.plot(F, L, label=\"Lorenz curve, wealth distribution\")\n",
    "    ax.legend()\n",
    "\n",
    "    if savefig:\n",
    "        fig.savefig(\"../figures/finite_opt_saving_lorenz.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timing(savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series(savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa96fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz(savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447188a",
   "metadata": {},
   "source": [
    "#### finite_lq.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ca060",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from quantecon import compute_fixed_point\n",
    "from quantecon.markov import tauchen, MarkovChain\n",
    "\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from numba import njit, prange\n",
    "import time\n",
    "\n",
    "\n",
    "# NamedTuple Model\n",
    "Model = namedtuple(\"Model\", (\"β\", \"a_0\", \"a_1\", \"γ\", \"c\",\n",
    "                             \"y_grid\", \"z_grid\", \"Q\"))\n",
    "\n",
    "\n",
    "def create_investment_model(\n",
    "        r=0.04,                               # Interest rate\n",
    "        a_0=10.0, a_1=1.0,                    # Demand parameters\n",
    "        γ=25.0, c=1.0,                        # Adjustment and unit cost\n",
    "        y_min=0.0, y_max=20.0, y_size=100,    # Grid for output\n",
    "        ρ=0.9, ν=1.0,                         # AR(1) parameters\n",
    "        z_size=25):                           # Grid size for shock\n",
    "    β = 1/(1+r)\n",
    "    y_grid = np.linspace(y_min, y_max, y_size)\n",
    "    mc = tauchen(y_size, ρ, ν)\n",
    "    z_grid, Q = mc.state_values, mc.P\n",
    "    return Model(β=β, a_0=a_0, a_1=a_1, γ=γ, c=c,\n",
    "          y_grid=y_grid, z_grid=z_grid, Q=Q)\n",
    "\n",
    "\n",
    "@njit\n",
    "def B(i, j, k, v, model):\n",
    "    \"\"\"\n",
    "    The aggregator B is given by\n",
    "\n",
    "        B(y, z, y′) = r(y, z, y′) + β Σ_z′ v(y′, z′) Q(z, z′).\"\n",
    "\n",
    "    where\n",
    "\n",
    "        r(y, z, y′) := (a_0 - a_1 * y + z - c) y - γ * (y′ - y)^2\n",
    "\n",
    "    \"\"\"\n",
    "    β, a_0, a_1, γ, c, y_grid, z_grid, Q = model\n",
    "    y, z, y_1 = y_grid[i], z_grid[j], y_grid[k]\n",
    "    r = (a_0 - a_1 * y + z - c) * y - γ * (y_1 - y)**2\n",
    "    return r + β * np.dot(v[k, :], Q[j, :])\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def T_σ(v, σ, model):\n",
    "    \"\"\"The policy operator.\"\"\"\n",
    "    v_new = np.empty_like(v)\n",
    "    for i in prange(len(model.y_grid)):\n",
    "        for j in prange(len(model.z_grid)):\n",
    "            v_new[i, j] = B(i, j, σ[i, j], v, model)\n",
    "    return v_new\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def T(v, model):\n",
    "    \"\"\"The Bellman operator.\"\"\"\n",
    "    v_new = np.empty_like(v)\n",
    "    for i in prange(len(model.y_grid)):\n",
    "        for j in prange(len(model.z_grid)):\n",
    "            tmp = np.array([B(i, j, k, v, model) for k\n",
    "                            in np.arange(len(model.y_grid))])\n",
    "            v_new[i, j] = np.max(tmp)\n",
    "    return v_new\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_greedy(v, model):\n",
    "    \"\"\"Compute a v-greedy policy.\"\"\"\n",
    "    n, m = len(model.y_grid), len(model.z_grid)\n",
    "    σ = np.empty((n, m), dtype=np.int32)\n",
    "    for i in prange(n):\n",
    "        for j in prange(m):\n",
    "            tmp = np.array([B(i, j, k, v, model) for k\n",
    "                            in np.arange(n)])\n",
    "            σ[i, j] = np.argmax(tmp)\n",
    "    return σ\n",
    "\n",
    "\n",
    "\n",
    "def value_iteration(model, tol=1e-5):\n",
    "    \"\"\"Value function iteration routine.\"\"\"\n",
    "    vz = np.zeros((len(model.y_grid), len(model.z_grid)))\n",
    "    v_star = compute_fixed_point(lambda v: T(v, model), vz,\n",
    "                                 error_tol=tol, max_iter=1000, print_skip=25)\n",
    "    return get_greedy(v_star, model)\n",
    "\n",
    "\n",
    "@njit\n",
    "def single_to_multi(m, zn):\n",
    "    # Function to extract (i, j) from m = i + (j-1)*zn\n",
    "    return (m//zn, m%zn)\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_value(σ, model):\n",
    "    \"\"\"Get the value v_σ of policy σ.\"\"\"\n",
    "    # Unpack and set up\n",
    "    β, a_0, a_1, γ, c, y_grid, z_grid, Q = model\n",
    "    yn, zn = len(y_grid), len(z_grid)\n",
    "    n = yn * zn\n",
    "    # Allocate and create single index versions of P_σ and r_σ\n",
    "    P_σ = np.zeros((n, n))\n",
    "    r_σ = np.zeros(n)\n",
    "    for m in prange(n):\n",
    "        i, j = single_to_multi(m, zn)\n",
    "        y, z, y_1 = y_grid[i], z_grid[j], y_grid[σ[i, j]]\n",
    "        r_σ[m] = (a_0 - a_1 * y + z - c) * y - γ * (y_1 - y)**2\n",
    "        for m_1 in prange(n):\n",
    "            i_1, j_1 = single_to_multi(m_1, zn)\n",
    "            if i_1 == σ[i, j]:\n",
    "                P_σ[m, m_1] = Q[j, j_1]\n",
    "\n",
    "    I = np.identity(n)\n",
    "    # Solve for the value of σ\n",
    "    v_σ = np.linalg.solve((I - β * P_σ), r_σ)\n",
    "    # Return as multi-index array\n",
    "    v_σ = v_σ.reshape(yn, zn)\n",
    "    return v_σ\n",
    "\n",
    "\n",
    "@njit\n",
    "def policy_iteration(model):\n",
    "    \"\"\"Howard policy iteration routine.\"\"\"\n",
    "    yn, zn = len(model.y_grid), len(model.z_grid)\n",
    "    σ = np.ones((yn, zn), dtype=np.int32)\n",
    "    i, error = 0, 1.0\n",
    "    while error > 0:\n",
    "        v_σ = get_value(σ, model)\n",
    "        σ_new = get_greedy(v_σ, model)\n",
    "        error = np.max(np.abs(σ_new - σ))\n",
    "        σ = σ_new\n",
    "        i = i + 1\n",
    "        print(f\"Concluded loop {i} with error: {error}.\")\n",
    "    return σ\n",
    "\n",
    "\n",
    "@njit\n",
    "def optimistic_policy_iteration(model, tol=1e-5, m=100):\n",
    "    \"\"\"Optimistic policy iteration routine.\"\"\"\n",
    "    v = np.zeros((len(model.y_grid), len(model.z_grid)))\n",
    "    error = tol + 1\n",
    "    while error > tol:\n",
    "        last_v = v\n",
    "        σ = get_greedy(v, model)\n",
    "        for i in range(m):\n",
    "            v = T_σ(v, σ, model)\n",
    "        error = np.max(np.abs(v - last_v))\n",
    "    return get_greedy(v, model)\n",
    "\n",
    "\n",
    "# Plots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"text.usetex\": True, \"font.size\": 14})\n",
    "\n",
    "\n",
    "def plot_policy(savefig=False, figname=\"../figures/finite_lq_0.pdf\"):\n",
    "    model = create_investment_model()\n",
    "    β, a_0, a_1, γ, c, y_grid, z_grid, Q = model\n",
    "    σ_star = optimistic_policy_iteration(model)\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "    ax.plot(y_grid, y_grid, \"k--\", label=r\"$45$\")\n",
    "    ax.plot(y_grid, y_grid[σ_star[:, 0]], label=r\"$\\sigma^*(\\cdot, z_1)$\")\n",
    "    ax.plot(y_grid, y_grid[σ_star[:, -1]], label=\"$\\sigma^*(\\cdot, z_N)$\")\n",
    "    ax.legend()\n",
    "    if savefig:\n",
    "        fig.savefig(figname)\n",
    "\n",
    "\n",
    "def plot_sim(savefig=False, figname=\"../figures/finite_lq_1.pdf\"):\n",
    "    ts_length = 200\n",
    "\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(9, 11.2))\n",
    "\n",
    "    for (ax, γ) in zip(axes, (1, 10, 20, 30)):\n",
    "        model = create_investment_model(γ=γ)\n",
    "        β, a_0, a_1, γ, c, y_grid, z_grid, Q = model\n",
    "        σ_star = optimistic_policy_iteration(model)\n",
    "        mc = MarkovChain(Q, z_grid)\n",
    "\n",
    "        z_sim_idx = mc.simulate_indices(ts_length)\n",
    "        z_sim = z_grid[z_sim_idx]\n",
    "\n",
    "        y_sim_idx = np.empty(ts_length, dtype=np.int32)\n",
    "        y_1 = (a_0 - c + z_sim[1]) / (2 * a_1)\n",
    "\n",
    "        y_sim_idx[0] = np.searchsorted(y_grid, y_1)\n",
    "        for t in range(ts_length-1):\n",
    "            y_sim_idx[t+1] = σ_star[y_sim_idx[t], z_sim_idx[t]]\n",
    "        y_sim = y_grid[y_sim_idx]\n",
    "        y_bar_sim = (a_0 - c + z_sim) / (2 * a_1)\n",
    "\n",
    "        ax.plot(np.arange(1, ts_length+1), y_sim, label=r\"$Y_t$\")\n",
    "        ax.plot(np.arange(1, ts_length+1), y_bar_sim, label=r\"$\\bar Y_t$\")\n",
    "        ax.legend(frameon=False, loc=\"upper right\")\n",
    "        ax.set_ylabel(\"output\")\n",
    "        ax.set_ylim(1, 9)\n",
    "        ax.set_title(r\"$\\gamma = $\" + f\"{γ}\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if savefig:\n",
    "        fig.savefig(figname)\n",
    "\n",
    "\n",
    "def plot_timing(m_vals=np.arange(1, 601, 10),\n",
    "                savefig=False,\n",
    "                figname=\"../figures/finite_lq_time.pdf\"\n",
    "    ):\n",
    "    # NOTE: Uncomment the following lines in this function to\n",
    "    # include Policy iteration plot\n",
    "    model = create_investment_model()\n",
    "    # print(\"Running Howard policy iteration.\")\n",
    "    # t1 = time.time()\n",
    "    # σ_pi = policy_iteration(model)\n",
    "    # pi_time = time.time() - t1\n",
    "    # print(f\"PI completed in {pi_time} seconds.\")\n",
    "    print(\"Running value function iteration.\")\n",
    "    t1 = time.time()\n",
    "    σ_vfi = value_iteration(model)\n",
    "    vfi_time = time.time() - t1\n",
    "    print(f\"VFI completed in {vfi_time} seconds.\")\n",
    "    opi_times = []\n",
    "    for m in m_vals:\n",
    "        print(f\"Running optimistic policy iteration with m={m}.\")\n",
    "        t1 = time.time()\n",
    "        σ_opi = optimistic_policy_iteration(model, m=m, tol=1e-5)\n",
    "        t2 = time.time()\n",
    "        print(f\"OPI with m={m} completed in {t2-t1} seconds.\")\n",
    "        opi_times.append(t2-t1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "    ax.plot(m_vals, [vfi_time]*len(m_vals),\n",
    "            linewidth=2, label=\"value function iteration\")\n",
    "    # ax.plot(m_vals, [pi_time]*len(m_vals),\n",
    "    #         linewidth=2, label=\"Howard policy iteration\")\n",
    "    ax.plot(m_vals, opi_times, linewidth=2, label=\"optimistic policy iteration\")\n",
    "    ax.legend(frameon=False)\n",
    "    ax.set_xlabel(r\"$m$\")\n",
    "    ax.set_ylabel(\"time\")\n",
    "    if savefig:\n",
    "        fig.savefig(figname)\n",
    "    return (vfi_time, opi_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c72875",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim(savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7686d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timing(savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac96a3",
   "metadata": {},
   "source": [
    "#### firm_hiring.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb949b8",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from quantecon.markov import tauchen, MarkovChain\n",
    "\n",
    "from collections import namedtuple\n",
    "from numba import njit, prange\n",
    "\n",
    "\n",
    "# NamedTuple Model\n",
    "Model = namedtuple(\"Model\", (\"β\", \"κ\", \"α\", \"p\", \"w\", \"l_grid\",\n",
    "                             \"z_grid\", \"Q\"))\n",
    "\n",
    "\n",
    "def create_hiring_model(\n",
    "        r=0.04,                              # Interest rate\n",
    "        κ=1.0,                               # Adjustment cost\n",
    "        α=0.4,                               # Production parameter\n",
    "        p=1.0, w=1.0,                        # Price and wage\n",
    "        l_min=0.0, l_max=30.0, l_size=100,   # Grid for labor\n",
    "        ρ=0.9, ν=0.4, b=1.0,                 # AR(1) parameters\n",
    "        z_size=100):                         # Grid size for shock\n",
    "    β = 1/(1+r)\n",
    "    l_grid = np.linspace(l_min, l_max, l_size)\n",
    "    mc = tauchen(z_size, ρ, ν, b, 6)\n",
    "    z_grid, Q = mc.state_values, mc.P\n",
    "    return Model(β=β, κ=κ, α=α, p=p, w=w,\n",
    "                 l_grid=l_grid, z_grid=z_grid, Q=Q)\n",
    "\n",
    "\n",
    "@njit\n",
    "def B(i, j, k, v, model):\n",
    "    \"\"\"\n",
    "    The aggregator B is given by\n",
    "\n",
    "        B(l, z, l′) = r(l, z, l′) + β Σ_z′ v(l′, z′) Q(z, z′).\"\n",
    "\n",
    "    where\n",
    "\n",
    "        r(l, z, l′) := p * z * f(l) - w * l - κ 1{l != l′}\n",
    "\n",
    "    \"\"\"\n",
    "    β, κ, α, p, w, l_grid, z_grid, Q = model\n",
    "    l, z, l_1 = l_grid[i], z_grid[j], l_grid[k]\n",
    "    r = p * z * l**α - w * l - κ * (l != l_1)\n",
    "    return r + β * np.dot(v[k, :], Q[j, :])\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def T_σ(v, σ, model):\n",
    "    \"\"\"The policy operator.\"\"\"\n",
    "    v_new = np.empty_like(v)\n",
    "    for i in prange(len(model.l_grid)):\n",
    "        for j in prange(len(model.z_grid)):\n",
    "            v_new[i, j] = B(i, j, σ[i, j], v, model)\n",
    "    return v_new\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_greedy(v, model):\n",
    "    \"\"\"Compute a v-greedy policy.\"\"\"\n",
    "    β, κ, α, p, w, l_grid, z_grid, Q = model\n",
    "    n, m = len(l_grid), len(z_grid)\n",
    "    σ = np.empty((n, m), dtype=np.int32)\n",
    "    for i in prange(n):\n",
    "        for j in prange(m):\n",
    "            tmp = np.array([B(i, j, k, v, model) for k\n",
    "                            in np.arange(n)])\n",
    "            σ[i, j] = np.argmax(tmp)\n",
    "    return σ\n",
    "\n",
    "\n",
    "@njit\n",
    "def optimistic_policy_iteration(model, tolerance=1e-5, m=100):\n",
    "    \"\"\"Optimistic policy iteration routine.\"\"\"\n",
    "    v = np.zeros((len(model.l_grid), len(model.z_grid)))\n",
    "    error = tolerance + 1\n",
    "    while error > tolerance:\n",
    "        last_v = v\n",
    "        σ = get_greedy(v, model)\n",
    "        for i in range(m):\n",
    "            v = T_σ(v, σ, model)\n",
    "        error = np.max(np.abs(v - last_v))\n",
    "    return get_greedy(v, model)\n",
    "\n",
    "\n",
    "# Plots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"text.usetex\": True, \"font.size\": 14})\n",
    "\n",
    "\n",
    "def plot_policy(savefig=False,\n",
    "                figname=\"../figures/firm_hiring_pol.pdf\"):\n",
    "    model = create_hiring_model()\n",
    "    β, κ, α, p, w, l_grid, z_grid, Q = model\n",
    "    σ_star = optimistic_policy_iteration(model)\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "    ax.plot(l_grid, l_grid, \"k--\", label=r\"$45$\")\n",
    "    ax.plot(l_grid, l_grid[σ_star[:, 0]], label=r\"$\\sigma^*(\\cdot, z_1)$\")\n",
    "    ax.plot(l_grid, l_grid[σ_star[:, -1]], label=r\"$\\sigma^*(\\cdot, z_N)$\")\n",
    "    ax.legend()\n",
    "    if savefig:\n",
    "        fig.savefig(figname)\n",
    "\n",
    "\n",
    "def sim_dynamics(model, ts_length):\n",
    "    β, κ, α, p, w, l_grid, z_grid, Q = model\n",
    "    σ_star = optimistic_policy_iteration(model)\n",
    "    mc = MarkovChain(Q, z_grid)\n",
    "    z_sim_idx = mc.simulate_indices(ts_length)\n",
    "    z_sim = z_grid[z_sim_idx]\n",
    "    l_sim_idx = np.empty(ts_length, dtype=np.int32)\n",
    "    l_sim_idx[0] = 32\n",
    "    for t in range(ts_length-1):\n",
    "        l_sim_idx[t+1] = σ_star[l_sim_idx[t], z_sim_idx[t]]\n",
    "    l_sim = l_grid[l_sim_idx]\n",
    "\n",
    "    y_sim = np.empty_like(l_sim)\n",
    "    for (i, l) in enumerate(l_sim):\n",
    "        y_sim[i] = p * z_sim[i] * l_sim[i]**α\n",
    "\n",
    "    t = ts_length - 1\n",
    "    l_g, y_g, z_g = np.zeros(t), np.zeros(t), np.zeros(t)\n",
    "\n",
    "    for i in range(t):\n",
    "        l_g[i] = (l_sim[i+1] - l_sim[i]) / l_sim[i]\n",
    "        y_g[i] = (y_sim[i+1] - y_sim[i]) / y_sim[i]\n",
    "        z_g[i] = (z_sim[i+1] - z_sim[i]) / z_sim[i]\n",
    "\n",
    "    return l_sim, y_sim, z_sim, l_g, y_g, z_g\n",
    "\n",
    "\n",
    "def plot_sim(savefig=False,\n",
    "             figname=\"../figures/firm_hiring_ts.pdf\",\n",
    "             ts_length = 250):\n",
    "    model = create_hiring_model()\n",
    "    β, κ, α, p, w, l_grid, z_grid, Q = model\n",
    "    l_sim, y_sim, z_sim, l_g, y_g, z_g = sim_dynamics(model, ts_length)\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "    x_grid = np.arange(ts_length)\n",
    "    ax.plot(x_grid, l_sim, label=r\"$\\ell_t$\")\n",
    "    ax.plot(x_grid, z_sim, alpha=0.6, label=r\"$Z_t$\")\n",
    "    ax.legend(frameon=False)\n",
    "    ax.set_ylabel(\"employment\")\n",
    "    ax.set_xlabel(\"time\")\n",
    "\n",
    "    if savefig:\n",
    "        fig.savefig(figname)\n",
    "\n",
    "\n",
    "def plot_growth(savefig=False,\n",
    "                figname=\"../figures/firm_hiring_g.pdf\",\n",
    "                ts_length = 10_000_000):\n",
    "\n",
    "    model = create_hiring_model()\n",
    "    β, κ, α, p, w, l_grid, z_grid, Q = model\n",
    "    l_sim, y_sim, z_sim, l_g, y_g, z_g = sim_dynamics(model, ts_length)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(l_g, alpha=0.6, bins=100)\n",
    "    ax.set_xlabel(\"growth\")\n",
    "\n",
    "    #fig, axes = plt.subplots(2, 1)\n",
    "    #series = y_g, z_g\n",
    "    #for (ax, g) in zip(axes, series):\n",
    "    #    ax.hist(g, alpha=0.6, bins=100)\n",
    "    #    ax.set_xlabel(\"growth\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        fig.savefig(figname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbcd097",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a59c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim(savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_growth(savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74694a",
   "metadata": {},
   "source": [
    "#### modified_opt_savings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30949e35",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from quantecon import tauchen, MarkovChain\n",
    "\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from numba import njit, prange\n",
    "from math import floor\n",
    "\n",
    "\n",
    "# NamedTuple Model\n",
    "Model = namedtuple(\"Model\", (\"β\", \"γ\", \"η_grid\", \"φ\",\n",
    "                             \"w_grid\", \"y_grid\", \"Q\"))\n",
    "\n",
    "\n",
    "def create_savings_model(β=0.98, γ=2.5,  \n",
    "                         w_min=0.01, w_max=20.0, w_size=100,\n",
    "                         ρ=0.9, ν=0.1, y_size=20,\n",
    "                         η_min=0.75, η_max=1.25, η_size=2):\n",
    "    η_grid = np.linspace(η_min, η_max, η_size)\n",
    "    φ = np.ones(η_size) * (1 / η_size)  # Uniform distributoin\n",
    "    w_grid = np.linspace(w_min, w_max, w_size)\n",
    "    mc = tauchen(y_size, ρ, ν)\n",
    "    y_grid, Q = np.exp(mc.state_values), mc.P\n",
    "    return Model(β=β, γ=γ, η_grid=η_grid, φ=φ, w_grid=w_grid,\n",
    "                 y_grid=y_grid, Q=Q)\n",
    "\n",
    "## == Functions for regular OPI == ##\n",
    "\n",
    "@njit\n",
    "def U(c, γ):\n",
    "    return c**(1-γ)/(1-γ)\n",
    "\n",
    "@njit\n",
    "def B(i, j, k, l, v, model):\n",
    "    \"\"\"\n",
    "    The function\n",
    "\n",
    "    B(w, y, η, w′) = u(w + y - w′/η)) + β Σ v(w′, y′, η′) Q(y, y′) ϕ(η′)\n",
    "\n",
    "    \"\"\"\n",
    "    β, γ, η_grid, φ, w_grid, y_grid, Q = model\n",
    "    w, y, η, w_1 = w_grid[i], y_grid[j], η_grid[k], w_grid[l]\n",
    "    c = w + y - (w_1 / η)\n",
    "    exp_value = 0.0\n",
    "    for m in prange(len(y_grid)):\n",
    "        for n in prange(len(η_grid)):\n",
    "            exp_value += v[l, m, n] * Q[j, m] * φ[n]\n",
    "    return U(c, γ) + β * exp_value if c > 0 else -np.inf\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def T_σ(v, σ, model):\n",
    "    \"\"\"The policy operator.\"\"\"\n",
    "    β, γ, η_grid, φ, w_grid, y_grid, Q = model\n",
    "    v_new = np.empty_like(v)\n",
    "    for i in prange(len(w_grid)):\n",
    "        for j in prange(len(y_grid)):\n",
    "            for k in prange(len(η_grid)):\n",
    "                v_new[i, j, k] = B(i, j, k, σ[i, j, k], v, model)\n",
    "    return v_new\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_greedy(v, model):\n",
    "    \"\"\"Compute a v-greedy policy.\"\"\"\n",
    "    β, γ, η_grid, φ, w_grid, y_grid, Q = model\n",
    "    w_n, y_n, η_n = len(w_grid), len(y_grid), len(η_grid)\n",
    "    σ = np.empty((w_n, y_n, η_n), dtype=np.int32)\n",
    "    for i in prange(w_n):\n",
    "        for j in prange(y_n):\n",
    "            for k in prange(η_n):\n",
    "                _tmp = np.array([B(i, j, k, l, v, model) for l\n",
    "                                in range(w_n)])\n",
    "                σ[i, j, k] = np.argmax(_tmp)\n",
    "    return σ\n",
    "\n",
    "\n",
    "def optimistic_policy_iteration(model, tolerance=1e-5, m=100):\n",
    "    \"\"\"Optimistic policy iteration routine.\"\"\"\n",
    "    β, γ, η_grid, φ, w_grid, y_grid, Q = model\n",
    "    w_n, y_n, η_n = len(w_grid), len(y_grid), len(η_grid)\n",
    "    v = np.zeros((w_n, y_n, η_n))\n",
    "    error = tolerance + 1\n",
    "    while error > tolerance:\n",
    "        last_v = v\n",
    "        σ = get_greedy(v, model)\n",
    "        for i in range(m):\n",
    "            v = T_σ(v, σ, model)\n",
    "        error = np.max(np.abs(v - last_v))\n",
    "        print(f\"OPI current error = {error}\")\n",
    "    return get_greedy(v, model)\n",
    "\n",
    "\n",
    "## == Functions for modified OPI == ##\n",
    "\n",
    "\n",
    "@njit\n",
    "def D(i, j, k, l, g, model):\n",
    "    \"\"\"D(w, y, η, w′, g) = u(w + y - w′/η) + β g(y, w′).\"\"\"\n",
    "    β, γ, η_grid, φ, w_grid, y_grid, Q = model\n",
    "    w, y, η, w_1 = w_grid[i], y_grid[j], η_grid[k], w_grid[l]\n",
    "    c = w + y - (w_1 / η)\n",
    "    return U(c, γ) + β * g[j, l] if c > 0 else -np.inf\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_g_greedy(g, model):\n",
    "    \"\"\"Compute a g-greedy policy.\"\"\"\n",
    "    β, γ, η_grid, φ, w_grid, y_grid, Q = model\n",
    "    w_n, y_n, η_n = len(w_grid), len(y_grid), len(η_grid)\n",
    "    σ = np.empty((w_n, y_n, η_n), dtype=np.int32)\n",
    "    for i in prange(w_n):\n",
    "        for j in prange(y_n):\n",
    "            for k in prange(η_n):\n",
    "                _tmp = np.array([D(i, j, k, l, g, model) for l\n",
    "                                in range(w_n)])\n",
    "                σ[i, j, k] = np.argmax(_tmp)\n",
    "    return σ\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def R_σ(g, σ, model):\n",
    "    \"\"\"The modified policy operator.\"\"\"\n",
    "    β, γ, η_grid, φ, w_grid, y_grid, Q = model\n",
    "    w_n, y_n, η_n = len(w_grid), len(y_grid), len(η_grid)\n",
    "    g_new = np.empty_like(g)\n",
    "    for j in prange(y_n):\n",
    "        for i_1 in prange(w_n):\n",
    "            out = 0.0\n",
    "            for j_1 in prange(y_n):\n",
    "                for k_1 in prange(η_n):\n",
    "                    out += D(i_1, j_1, k_1, σ[i_1, j_1, k_1], g,\n",
    "                             model) * Q[j, j_1] * φ[k_1]\n",
    "            g_new[j, i_1] = out\n",
    "    return g_new\n",
    "\n",
    "\n",
    "def mod_opi(model, tolerance=1e-5, m=100):\n",
    "    \"\"\"Modified optimistic policy iteration routine.\"\"\"\n",
    "    β, γ, η_grid, φ, w_grid, y_grid, Q = model\n",
    "    g = np.zeros((len(y_grid), len(w_grid)))\n",
    "    error = tolerance + 1\n",
    "    while error > tolerance:\n",
    "        last_g = g\n",
    "        σ = get_g_greedy(g, model)\n",
    "        for i in range(m):\n",
    "            g = R_σ(g, σ, model)\n",
    "        error = np.max(np.abs(g - last_g))\n",
    "        print(f\"OPI current error = {error}\")\n",
    "    return get_g_greedy(g, model)\n",
    "\n",
    "\n",
    "def simulate_wealth(m):\n",
    "\n",
    "    model = create_savings_model()\n",
    "    σ_star = mod_opi(model)\n",
    "    β, γ, η_grid, φ, w_grid, y_grid, Q = model\n",
    "\n",
    "    # Simulate labor income\n",
    "    mc = MarkovChain(Q)\n",
    "    y_idx_series = mc.simulate(ts_length=m)\n",
    "\n",
    "    # IID Markov chain with uniform draws\n",
    "    l = len(η_grid)\n",
    "    mc = MarkovChain(np.ones((l, l)) / l)\n",
    "    η_idx_series = mc.simulate(ts_length=m)\n",
    "\n",
    "    w_idx_series = np.empty_like(y_idx_series)\n",
    "    w_idx_series[0] = 1  # initial condition\n",
    "    for t in range(m-1):\n",
    "        i, j, k = w_idx_series[t], y_idx_series[t], η_idx_series[t]\n",
    "        w_idx_series[t+1] = σ_star[i, j, k]\n",
    "    w_series = w_grid[w_idx_series]\n",
    "\n",
    "    return w_series\n",
    "\n",
    "def lorenz(v):  # assumed sorted vector\n",
    "    S = np.cumsum(v)  # cumulative sums: [v[1], v[1] + v[2], ... ]\n",
    "    F = np.arange(1, len(v) + 1) / len(v)\n",
    "    L = S / S[-1]\n",
    "    return (F, L) # returns named tuple\n",
    "\n",
    "gini = lambda v: (2 * sum(i * y for (i, y) in enumerate(v))/sum(v) - (len(v) + 1))/len(v)\n",
    "\n",
    "# Plots\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"text.usetex\": True, \"font.size\": 14})\n",
    "\n",
    "\n",
    "def plot_contours(savefig=False,\n",
    "                  figname=\"../figures/modified_opt_savings_1.pdf\"):\n",
    "\n",
    "    model = create_savings_model()\n",
    "    β, γ, η_grid, φ, w_grid, y_grid, Q = model\n",
    "    σ_star = optimistic_policy_iteration(model)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    y_n, η_n = len(y_grid), len(η_grid)\n",
    "    y_idx, η_idx = np.arange(y_n), np.arange(η_n)\n",
    "    H = np.zeros((y_n, η_n))\n",
    "\n",
    "    w_indices = (0, len(w_grid)-1)\n",
    "    titles = \"low wealth\", \"high wealth\"\n",
    "    for (ax, w_idx, title) in zip(axes, w_indices, titles):\n",
    "\n",
    "        for i_y in y_idx:\n",
    "            for i_η in η_idx:\n",
    "                w, y, η = w_grid[w_idx], y_grid[i_y], η_grid[i_η]\n",
    "                H[i_y, i_η] = w_grid[σ_star[w_idx, i_y, i_η]] / (w + y)\n",
    "\n",
    "        cs1 = ax.contourf(y_grid, η_grid, np.transpose(H), alpha=0.5)\n",
    "\n",
    "        plt.colorbar(cs1, ax=ax) #, format=\"%.6f\")\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(r\"$y$\")\n",
    "        ax.set_ylabel(r\"$\\varepsilon$\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        fig.savefig(figname)\n",
    "\n",
    "def plot_policies(savefig=False):\n",
    "    model = create_savings_model()\n",
    "    β, γ, η_grid, φ, w_grid, y_grid, Q = model\n",
    "    σ_star = mod_opi(model)\n",
    "    y_bar = floor(len(y_grid) / 2) # index of mid-point of y_grid\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "    ax.plot(w_grid, w_grid, \"k--\", label=r\"$45$\")\n",
    "\n",
    "    for (i, η) in enumerate(η_grid):\n",
    "        label = r\"$\\sigma^*$\" + \" at \" + r\"$\\eta = $\" + f\"{η.round(2)}\"\n",
    "        ax.plot(w_grid, w_grid[σ_star[:, y_bar, i]], label=label)\n",
    "        \n",
    "    ax.legend()\n",
    "    if savefig:\n",
    "        fig.savefig(f\"../figures/modified_opt_saving_2.pdf\")\n",
    "\n",
    "def plot_time_series(m=2_000, savefig=False):\n",
    "\n",
    "    w_series = simulate_wealth(m)\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "    ax.plot(w_series, label=r\"$w_t$\")\n",
    "    ax.set_xlabel(\"time\")\n",
    "    ax.legend()\n",
    "    if savefig:\n",
    "        fig.savefig(\"../figures/modified_opt_saving_ts.pdf\")\n",
    "\n",
    "def plot_histogram(m=1_000_000, savefig=False):\n",
    "\n",
    "    w_series = simulate_wealth(m)\n",
    "    w_series.sort()\n",
    "    g = round(gini(w_series), ndigits=2)\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "    ax.hist(w_series, bins=40, density=True)\n",
    "    ax.set_xlabel(\"wealth\")\n",
    "    ax.text(15, 0.4, f\"Gini = {g}\")\n",
    "\n",
    "    if savefig:\n",
    "        fig.savefig(\"../figures/modified_opt_saving_hist.pdf\")\n",
    "\n",
    "def plot_lorenz(m=1_000_000, savefig=False):\n",
    "\n",
    "    w_series = simulate_wealth(m)\n",
    "    w_series.sort()\n",
    "    (F, L) = lorenz(w_series)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 5.2))\n",
    "    ax.plot(F, F, label=\"Lorenz curve, equality\")\n",
    "    ax.plot(F, L, label=\"Lorenz curve, wealth distribution\")\n",
    "    ax.legend()\n",
    "\n",
    "    if savefig:\n",
    "        fig.savefig(\"../figures/modified_opt_saving_lorenz.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f08e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e020fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policies(savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series(savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68070320",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz(savefig=True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   10,
   30,
   163,
   167,
   169,
   171,
   234,
   236,
   291,
   293,
   479,
   483,
   487,
   491,
   495,
   497,
   499,
   752,
   756,
   760,
   762,
   764,
   941,
   945,
   949,
   951,
   953,
   1235,
   1239,
   1243,
   1247,
   1251
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}